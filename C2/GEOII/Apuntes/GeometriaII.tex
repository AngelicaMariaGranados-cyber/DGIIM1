\documentclass[x11names,table]{report}

%Paquetes de siempre
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}

%\usepackage{fontspec}

%SE LIA
% Setup the matha and mathx font (from mathabx.sty)
\DeclareFontFamily{U}{matha}{\hyphenchar\font45}
\DeclareFontShape{U}{matha}{m}{n}{
      <5> <6> <7> <8> <9> <10> gen * matha
      <10.95> matha10 <12> <14.4> <17.28> <20.74> <24.88> matha12
      }{}
\DeclareSymbolFont{matha}{U}{matha}{m}{n}
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
      <5> <6> <7> <8> <9> <10>
      <10.95> <12> <14.4> <17.28> <20.74> <24.88>
      mathx10
      }{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}

\DeclareMathSymbol{\obot}         {2}{matha}{"6B}
\DeclareMathSymbol{\bigobot}       {1}{mathx}{"CB}
%SE LIÓ

%Paquetes matemáticos
\usepackage{amsmath, amssymb}

%Paquetes auxiliares
\usepackage{soul} %Para subrayar
\usepackage[usenames, dvipsnames]{xcolor} %Para usar colores
\usepackage{wrapfig} %Para cuadrar tablas e imagenes en texto
\usepackage{multirow} %Para combinar filas
\usepackage{graphicx} %Para incluir imágenes
\usepackage{longtable}
\usepackage{blindtext}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\rad}{Rad}

\title{Geometría II}
\author{Juan Antonio Villegas Recio}
\date{}

\parskip=0.3cm

\begin{document}
\maketitle
\chapter{DIAGONALIZACIÓN DE ENDOMORFISMOS}

\section{Introducción}

Dados $V$ y $V'$ e.v. sobre un cuerpo $K$ ($K$ normalmente será $\mathbb{R}$ o $\mathbb{C}$).

Sean $B=\{u_1, \dots ,u_n\}$ y $B'=\{u'_1, \dots ,u'_m\}$ bases de V y V'.

Sea $f:V\longrightarrow V'$ una aplicación lineal.


\[
M(f,B,B')=\begin{pmatrix}
a_{11} & a_{1j} & a_{1n}\\
\vdots & \vdots & \vdots\\
a_{m1} & a_{mj} & a_{mn}\\
\end{pmatrix}_{m\times n}
\]

donde $f(u_j)=\sum_{i=1}^{m} a_{ij}u'_i$

Consideramos ahora $B_1$, $B_2$ bases de $V$ y $B'_1$ y $B'_2$ bases de $V'$, se verifica que:

$M(f,B_1,B'_1)=M(I_{V'},B'_2,B'_1)M(f,B_2,B'_2)M(I_V,B_1,B_2)$
\medskip

Sea ahora $f\in End(V)$: $M(f,B,B)=M(f,B)\in \mathcal{M}_n(K)$
\medskip

Si $B_1$ y $B_2$ son dos bases de V entonces tenemos:

\medskip
\[ 
M(f,B_1)=\underbrace{M(I_{V},B_2,B_1)}_{P^{-1}}M(f,B_2)\underbrace{M(I_V,B_1,B_2)}_P
\]\[
M(f,B_1)=P^{-1}M(f,B_2)P, \hspace{1cm} P \text{ regular}
\]
$M(f,B_1)\text{ y } M(f,B_2)$ son matrices semejantes.

Recordemos entonces que las dos matrices tienen el mismo rango, la misma traza, y el mismo determinante.

Esto permitía definir $\det(f)=\det(M(f,B)) \text{ y } \text{traza}(f)=\text{traza}(M(f,B))$ fijada una base.

Definimos también:

\[
\Phi_B:End(V)\longrightarrow \mathcal{M}_n(K)
\]\[
\hspace{1cm} f\longmapsto M(f,B)
\]
que es un isomorfismo

\textbf{Problema}: Dado un endomorfismo $f\in End(V)$, ¿existe $B$ de $V$ tal que $M(f,B)$ es diagonal?

\textbf{Definición}: Diremos que un endomorfismo $f\in End(V)$ es DIAGONALIZABLE si existe una base B de V tal que $M(f,B)$ es diagonal.

En el contexto de las matrices cuadradas la pregunta sería:

Dada una matriz $A\in \mathcal{M}_n(K)$, ¿es semejante $A$ a una matriz diagonal?

\textbf{Definición}: Diremos que una matriz cuadrada A es diagonalizable si es semejante a una matriz diagonal.


\textbf{Teorema}: Dada $f\in End(V)$. Las siguientes afirmaciones son equivalentes:
\begin{enumerate}
\item $f$ es una proyección.
\item $f\circ f=f$
\item $V=\ker(f-I_V)\oplus \ker(f)$
\item Existe una base $B$ de $V$ tal que:
\[M(f,B)=\left(
\begin{array}{c|c}
I_k & 0_{k\times(n-k)}\\ \hline
0_{(n-k)\times k} & 0_{(n-k)\times(n-k)}\\
\end{array}
\right)
\]
\end{enumerate}

\textbf{Teorema}: Dada $f\in End(V)$. Las siguientes afirmaciones son equivalentes:
\begin{enumerate}
\item $f$ es una simetría.
\item $f\circ f=I_V$
\item $V=\ker(f-I_V)\oplus \ker(f+I_V)$
\item Existe una base $B$ de $V$ tal que:
\[M(f,B)=\left(
\begin{array}{c|c}
I_k & 0_{k\times(n-k)}\\ \hline
0_{(n-k)\times k} & -I_{n-k}\\
\end{array}
\right)
\]
\end{enumerate}

\section{Valores y vectores propios. Subespacios propios}

\textbf{Definición}: Sea $f\in End(V)$, $V$ e.v sobre $K$

Diremos que $\lambda\in K$ es un \textbf{autovalor} o \textbf{valor propio} de $f$ si existe $v\in V\backslash\{0\} \text{ tal que } f(v)=\lambda v.$

En este caso a $v$ se le denomina \textbf{vector propio} o \textbf{autovector} de $f$ asociado a $\lambda$.

Sea $A\in \mathcal{M}_n(K)$. Diremos que $\lambda\in K$ es un \textbf{valor propio} o \textbf{autovalor} de $A$ si existe $x=(x_1, \dots , x_n)\in K^n$ tal que:

\[A\begin{pmatrix}
x_1\\ \vdots \\ x_n \\
\end{pmatrix} = \lambda \begin{pmatrix}
x_1\\ \vdots \\ x_n \\
\end{pmatrix}\Leftrightarrow Ax=\lambda x
\]

En este caso, $x$ es un \textbf{vector propio} o \textbf{autovector} de $A$ asociado a $\lambda$.

\textbf{Observaciones}:\begin{itemize}
\item Si $v$ es un vector propio de $f\in End(V)$ asociado a $\lambda$ y $B$ es una base de $V$, entonces $v_B$ es un vector propio de $A=M(f,B)$ asociado a $\lambda$.
\[f(v)=\lambda v \hspace{3cm} M(f,B)v_B=\lambda v_B\]
\item Si $x$ es un vector propio de $A\in \mathcal{M}_n(K)$ asociado a $\lambda$ entoncs el vector $v$ cuyas coordenadas en la base $B$ son $x$ es un vector propio de $f_A$ asociado a $\lambda$
\item Un vector propio sólo puede estar asociado a un único valor propio.

\end{itemize}
\textbf{Definición}

Denotaremos $\mathbf{V_\lambda}=\ker(f-\lambda I_V)$ y lo denominaremos el \textbf{subespacio propio} de $f$ asociado a $\lambda$.
\[V_\lambda=\ker(f-\lambda I_V)=\{v\in V | f(v)=\lambda v\}=\{\text{vectores propios de }f\text{ asociados a }\lambda\}\cup\{0\}
\]

Denotaremos $\mathbf{g_\lambda}=\dim(V_\lambda)$ y le llamaremos \textbf{multiplicidad geométrica} de $\lambda$

\textbf{Proposición}:

Sean $V$ e.v. y $f\in End(V)$. Sean $\{\lambda_1,\dots,\lambda_m\}$ valores propios de $f$ distintos. Si $v_i$ es un vector propio asociado al valor propio $\lambda_i, i=\{i,\dots, m\}$, entonces $\{v_1,\dots,v_m\}$ son l.i.

\textbf{Corolario}:
Si $V$ es un espacio vectorial de dimensión $n$ y $f\in End(V)$ entonces $f$ tiene como máximo $n$ valores propios distintos. Además, si $f$ tiene $n$ valores propios distintos, entonces $f$ es diagonalizable.

\textbf{Corolario}: 
Sean $\lambda_1,\dots,\lambda_m$ valores propios distintos de $f\in End(V)$, entonces:
\[V_{\lambda_1}+\dots+V_{\lambda_m}=V_{\lambda_1}\oplus\dots\oplus V_{\lambda_m}\]

\section{Polinomio característico. Multiplicidad algebraica}

Para endomorfismos: $V$ e.v. y $f\in End(V)$

$\lambda$ es un valor propio de $f \Leftrightarrow V_\lambda=\ker(f-\lambda I_V)\not=\{0\} \Leftrightarrow det(f-\lambda I_V)=0$

$\det(f-\lambda I_V)$ es un polinomio en $\lambda$ de grado $n=\dim(V)$.
Denotaremos $P_f(\lambda)$ a ese polinomio y lo llamaremos \textbf{polinomio característico} de $f$. A la ecuación $P_f(\lambda)=0$ le llamaremos \textbf{ecuación característica} de $f$.

Los valores propios de $f$ son las raíces del polinomio característico, es decir, las soluciones de la ecuación característica.

Si $\lambda$ es un valor propio de $f$ entonces $P_f(\lambda)=(\lambda-\mu)^k q(\lambda)$. A k se le llama \textbf{multiplicidad algebraica} del valor propio $\mu$ y lo denotaremos $\mathbf{a_\mu}$

Para matrices cuadradas: $A\in\mathcal{M}_n(K)$
\[
\begin{split}
\mathbf{\lambda} \text{\textbf{ es un valor propio de }}\mathbf{A\in\mathcal{M}_n(K)} &\Leftrightarrow \text{Existe } x\in K^n-\{0\} \text{ t.q. } Ax=\lambda x\\
&\Leftrightarrow\text{Existe } x\in K^n-\{0\} \text{ t.q. } Ax-\lambda\cdot I_n\cdot x=0\\
&\Leftrightarrow\text{Existe } x\in K^n-\{0\} \text{ t.q. }(A-\lambda\cdot I_n)x=0\\
&\Leftrightarrow\mathbf{\det(A-\lambda\cdot I_n)=0}\\
\end{split}
\]

Denotaremos $P_A(\lambda)=\det(A-\lambda\cdot I_n)$ y le denominaremos \textbf{polinomio característico} de A. También llamaremos \textbf{ecuación característica} de A a la ecuación $\det(A-\lambda\cdot I_n)=0$

Los valores propios de $A$ son las raíces del polinomio característico, es decir, las soluciones de la ecuación característica.

Si $\lambda$ es un valor propio de $A$ entonces $P_A(\lambda)=(\lambda-\mu)^k q(\lambda)$. A k se le llama \textbf{multiplicidad algebraica} del valor propio $\mu$ y lo denotaremos $\mathbf{a_\mu}$

El polinomio característico de un endomorfismo coincide con el de su matriz en cualquier base. Sea $B$ una base de $V$:
\[
P_f(\lambda)=\det(f-\lambda I_V)=\det(M(f-\lambda I_V,B))=\det(M(f,B)-\lambda I_n)=P_{M(f,B)}(\lambda)
\]\(
A\in\mathcal{M}_n(K)
\)\[
P_A(\lambda)=\det(A-\lambda I_n)=\det(M(f_A,B)-\lambda I_n)=det(M(f_A-\lambda I_V,B))=P_{f_A}(\lambda)
\]

\textbf{Proposición}:

Sea $f\in End(V)$ y $\lambda$ valor propio de $f$. Entonces $\mathbf{g_\lambda\leq a_\lambda}$

\section{El teorema fundamental de la diagonalización}
Sea $V$ un e.v. y sea $f\in End(V)$
\[
\begin{split}
f \text{ es diagonalizable } &\Leftrightarrow \text{ Existe } B=\{u_1,\cdots,u_n\} \text{ base de } V \text{ t.q. } M(f,B) \text{ es diagonal }\\
&\Leftrightarrow \text{ Existe } B \text{ base de } V \text{ formada por vectores propios de f.}\\
\end{split}
\]
Si $f$ es diagonalizable, entonces existe $B=\{u_1,\dots,u_n\}$ base de $V$ t.q.
\[
M(f,B)=\begin{pmatrix}
\lambda_{i1}&0&\cdots&0\\
0&\lambda_{i2}&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&\lambda_{in}\\
\end{pmatrix}, \lambda_{ij}\in\{\lambda_1,\dots,\lambda_k\}
\]
Supongamos que $\{\lambda_1,\dots,\lambda_k\}$ son los valores propios de $f$.
\[
P_f(\lambda)=\det\left(
\begin{pmatrix}
\lambda_{i1}&0&\cdots&0\\
0&\lambda_{i2}&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&\lambda_{in}\\
\end{pmatrix}-\lambda\begin{pmatrix}
1&0&\cdots&0\\
0&1&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&1\\
\end{pmatrix}\right)=
\]\[
\det\begin{pmatrix}
\lambda_{i1}-\lambda&0&\cdots&0\\
0&\lambda_{i2}-\lambda&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&\lambda_{in}-\lambda\\
\end{pmatrix}=(\lambda_1-\lambda)^{a_{\lambda_1}}\cdots(\lambda_k-\lambda)^{a_{\lambda_k}}
\]

Por tanto, $P_f(\lambda)$ tiene $n$ raíces, contando la multiplicidad. Por otra parte:

\[\mathbf{g_{\lambda_i}}=\dim(V_{\lambda_i})=n-rg
\begin{pmatrix}
\lambda_{i1}-\lambda_i&0&\cdots&0\\
0&\lambda_{i2}-\lambda_i&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&\lambda_{in}-\lambda_i\\
\end{pmatrix}=n-(n-a_{\lambda_i}=\mathbf{a_{\lambda_i}}
\]

\subsection{Teorema fundamental de la diagonalización de endomorfismos}

Sea $V$ e.v. sobre $K$ y $f\in End(V)$. Equivalen las siguientes afirmaciones:
\begin{enumerate}
\renewcommand{\labelenumi}{\roman{enumi})}
\item $f$ es diagonalizable.
\item Existe una base de $V$ formada por vectores propios de $f$.
\item Si  $\{\lambda_1,\dots,\lambda_k\}$ son los distintos valores propios de $f$, entonces:
\begin{itemize}
\item $P_f(\lambda)$ tiene $n$ raíces (contando la multiplicidad, $\sum_{i=1}^k a_{\lambda_i}=n$)
\item $g_{\lambda_i}=a_{\lambda_i}, i=1,\dots,k$
\end{itemize}
\item $V=V_{\lambda_1}\oplus\dots\oplus V_{\lambda_m}$, donde $\{\lambda_1,\dots,\lambda_k\}$ son los distintos valores propios de $f$.
\end{enumerate}
Análogamente tendríamos el resultado para matrices:

\subsection{Teorema fundamental de la diagonalización de matrices}

Sea $A\in \mathcal{M}_n(K)$. Las siguientes afirmaciones son equivalentes:
\begin{enumerate}
\renewcommand{\labelenumi}{\roman{enumi})}
\item $A$ es diagonalizable.
\item Existe una base de $K^n$ formada por vectores propios de $A$.
\item Si  $\{\lambda_1,\dots,\lambda_k\}$ son los distintos valores propios de $A$, entonces:
\begin{itemize}
\item $P_A(\lambda)$ tiene $n$ raíces (contando la multiplicidad, $\sum_{i=1}^k a_{\lambda_i}=n$)
\item $g_{\lambda_i}=a_{\lambda_i}, i=1,\dots,k$
\end{itemize}
\item $K^n=V_{\lambda_1}\oplus\dots\oplus V_{\lambda_m}$, donde $\{\lambda_1,\dots,\lambda_k\}$ son los distintos valores propios de $A$.
\end{enumerate}

\textbf{Corolario}:

Sean $A,C\in \mathcal{M}_n(K)$ \textbf{diagonalizables}.

Entonces $A$ y $C$ son \textbf{semejantes} si y sólo si tienen el mismo polinomio característico, es decir, $P_A(\lambda)=P_C(\lambda)$


\section{El teorema de Cayley-Hamilton. Aplicaciones.}

Sea $A\in\mathcal{M}_n(K)$ y sea 

$P_A(\lambda)=\det(A-\lambda I_n)=(-1)^n\lambda^n+c_{n-1}\lambda^{n-1}+\dots +c_1\lambda+c_0,\, c_i\in K$

Denotaremos

$P_A(A)=(-1)^nA^n+c_{n-1}A^{n-1}+\dots +c_1A+c_0 I_n$

\subsection{Teorema de Cayley-Hamilton}

Sea $A\in\mathcal{M}_n(K)$ y $P_A(\lambda)=(-1)^n\lambda^n+c_{n-1}\lambda^{n-1}+\dots +c_1\lambda+c_0$, con $c_i\in K, i=0,\dots,n-1$. Entonces $P_A(A)=0_n$, donde $0_n$ es la matriz nula de orden $n$, es decir, cada matriz cuadrada es solución de su ecuación característica.

Análogamente se tendría el resultado para endomorfismos:

Sea $f\in End(V), V$ e.v. sobre $K$ y 
$P_f(\lambda)=(-1)^n\lambda^n+c_{n-1}\lambda^{n-1}+\dots +c_1\lambda+c_0$, con $c_i\in K, i=0,\dots,n-1$. Entonces $P_f(f)=f_0$, donde $f_0$ es el endomorfismo idénticamente nulo.

\subsection{Aplicaciones}

Sea $A\in\mathcal{M}_n(K)$. Si A es diagonalizable sabemos que existe $P$ una matriz regular tal que:
\[ P^{-1}\cdot A\cdot P=D=
\begin{pmatrix}
\lambda_1&0&\cdots&0\\
0&\lambda_2&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&\lambda_n\\
\end{pmatrix}\]
\[(P^{-1}\cdot A\cdot P)^n=P^{-1}\cdot A^n\cdot P=D^n=\begin{pmatrix}
\lambda_1^n&0&\cdots&0\\
0&\lambda_2^n&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&\lambda_n^n\\
\end{pmatrix}\]
\begin{center}
\fcolorbox{black}{white}{$A^n=P\cdot D^n\cdot P^{-1}$}
\end{center}


Si $A$ es regular:
\[ 
\begin{split}
(P^{-1}\cdot A\cdot P)^{-1}=D^{-1}=
\begin{pmatrix}
\frac{1}{\lambda_1}&0&\cdots&0\\
0&\frac{1}{\lambda_2}&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&\frac{1}{\lambda_n}\\
\end{pmatrix}&\Leftrightarrow\\
P^{-1}\cdot A^{-1}\cdot P=D^{-1}&\Leftrightarrow\\
P^{-1}\cdot A^{-n}\cdot P=D^{-n}&\Leftrightarrow\\
\fcolorbox{black}{white}{$A^{-n}=P\cdot D^{-n}\cdot P^{-1}$}& \\
\end{split}\]

\chapter{FORMAS BILINEALES Y CUADRÁTICAS}

\section{Definiciones y ejemplos. Expresión matricial. Congruencia de matrices}
\subsection*{Definición}
Dado $V(\mathbb{R})$ un e.v., se tiene una forma bilineal $b:V\times V\longrightarrow\mathbb{R}$ que cumple las siguientes propiedades:
\begin{itemize}
\item $b(\lambda\vec{u}+\mu\vec{v},\vec{w})=\lambda b(\vec{u},\vec{w})+\mu b(\vec{v},\vec{w})\hspace{1cm} \forall\vec{u},\vec{v},\vec{w}\in V,\  \forall \lambda,\mu\in\mathbb{R}$
\item $b(\vec{u},\lambda\vec{v}+\mu\vec{w})=\lambda b(\vec{u},\vec{v})+\mu b(\vec{u},\vec{w})\hspace{1cm} \forall\vec{u},\vec{v},\vec{w}\in V,\  \forall \lambda,\mu\in\mathbb{R}$
\end{itemize}
Denotaremos $\mathcal{B}(V)$ al conjunto de formas bilineales de V.

\subsection*{Ejemplos}
\subsubsection*{En $\mathbb{R}^n$}

Dada $A\in \mathcal{M}_n(\mathbb{R})$, se tiene la aplicación:
\[
\begin{split}
b_A:\mathbb{R}^n\times\mathbb{R}^n &\longrightarrow\mathbb{R}\\
(\vec{u},\vec{v})&\longmapsto\vec{u}\cdot A\cdot\vec{v} \hspace{1cm} \forall \vec{u},\vec{v}\in\mathbb{R}^n
\end{split}
\]
$b(\lambda\vec{u}+\mu\vec{v},\vec{w})=\lambda b(\vec{u},\vec{w})+\mu b(\vec{v},\vec{w})=\lambda\vec{u}A\vec{w}+\mu\vec{v}A\vec{w}=(\lambda\vec{u}+\mu \vec{v})A\vec{w}$

$\forall\vec{u},\vec{v},\vec{w}\in V,\  \forall \lambda,\mu\in\mathbb{R}$
\subsubsection*{En $\mathcal{M}_n(\mathbb{R})$}

Dada $M\in \mathcal{M}_n(\mathbb{R})$, se definen las aplicaciones:
\[
b_1,b_2,b_3,b_4: \mathcal{M}_n(\mathbb{R})\times\mathcal{M}_n(\mathbb{R}) \longrightarrow\mathbb{R}
\]\[
b_1(X,Y)=\tr(XMY)\]\[
b_2(X,Y)=\tr(XMY^t)\]\[
b_3(X,Y)=\tr(X^tMY)\]\[
b_4(X,Y)=\tr(X^tMY^t)
\]
$\forall X,Y\in \mathcal{M}_n(\mathbb{R})$

Demostremos que $b_4$ es bilineal:

Por un lado:

$b_4(\lambda X+\mu Y,Z)=\tr((\lambda X+\mu Y)^tMZ^t)$ \shortstack{\scriptsize{\text{tr es lineal}}\\=} $\lambda\tr(X^tMZ^t)+\mu\tr(Y^tMZ^t)$

Por otro:

$b_4(\lambda X+\mu Y,Z)=\lambda \tr(X^tMZ^t)+\mu\tr(Y^tMZ^t)$

\subsubsection*{En $\mathbb{R}_n[x]$}

Sea $f(x)$ una función continua en $[a,b]$, se define la aplicación:
\[
\begin{split}
b: \mathbb{R}_n[x]\times\mathbb{R}_n[x]&\longrightarrow\mathbb{R}\\
(p(x),q(x))&\longmapsto\int_a^b p(x)q(x)f(x)dx\\
\end{split}
\]$\forall\ \lambda,\mu\in\mathbb{R},\forall\ p(x),q(x)\in \mathbb{R}_n[x]$
Demostremos que $b$ es bilineal

Por un lado:
\[
b(\lambda p(x)+\mu q(x),r(x))=\lambda b(p(x),r(x))+\mu b(q(x),r(x))=\]\[\lambda\int^b_a p(x)r(x)f(x)dx+\mu\int_a^b q(x)r(x)f(x)dx
\]

Por otro:
\[
b(\lambda p(x)+\mu q(x),r(x))=\int_a^b(\lambda p(x)+\mu q(x))r(x)f(x)dx=\]\[\int_a^b(\lambda p(x)r(x)f(x)+\mu q(x)r(x)f(x))dx=\]\[\lambda\int^b_a p(x)r(x)f(x)dx+\mu\int_a^b q(x)r(x)f(x)dx
\]

\subsection*{Expresión matricial}

Dado un e.v. $V(\mathbb{R})$ de dimensión $n$ y $B=(\vec{v_1},\dots,\vec{v_n})$ una base ordenada de V, tomamos $\vec{u},\vec{v}\in V$ tales que:
\[
\vec{u}=(a_1,\dots,a_n)_B \text{\hspace{1cm} y \hspace{1cm}} \vec{v}=(b_1,\dots,b_n)_B
\]
Dada $b\in\mathcal{B}(V)$, se sabe que:
\begin{align*}
b(\vec{u},\vec{v})=b\left(\sum^n_{i=1} a_i\vec{v_i},\sum^n_{j=1} b_i\vec{v_j}\right)=
\sum^n_{i=1} a_i b\left(\vec{v_i}, \sum^n_{j=1} b_i\vec{v_j}\right)= \\
\sum^n_{i=1}a_i\left(\sum^n_{j=1}b_j b(\vec{v_i},\vec{v_j})\right)=\sum^n_{i,j=1}a_i b_j b(\vec{v_i},\vec{v_j})
\end{align*}

Denotaremos $M(b,B)=(b(\vec{v_i},\vec{v_j}))_{1\leq i,j \leq n}\in \mathcal{M}_n(\mathbb{R})$ a la matriz siguiente:
\[
\begin{pmatrix}
b(\vec{v_1},\vec{v_1}) & b(\vec{v_1},\vec{v_2}) & \cdots & b(\vec{v_1},\vec{v_n})\\
b(\vec{v_2},\vec{v_1}) & b(\vec{v_2},\vec{v_2}) & \cdots & b(\vec{v_2},\vec{v_n})\\
\vdots & \vdots & \ddots & \vdots \\
b(\vec{v_n},\vec{v_1}) & b(\vec{v_n},\vec{v_2}) & \cdots & b(\vec{v_n},\vec{v_n})
\end{pmatrix}
\]




Observemos que se cumple que $b(\vec{u},\vec{v})=\vec{u}_B^t\cdot M(b,B)\cdot\vec{v}_B$ donde $\vec{u_B}$ y $\vec{v_B}$ son las coordenadas de los vectores $\vec{u}$ y $\vec{v}$ en la base $B$ expresadas como matriz columna. A esta se le llama \textbf{expresión matricial de la forma lineal $\mathbf{b}$ respecto a una base B}.

Dos formas bilineales $b,b'\in\mathcal{B}(V)$ cumplen que, fijada una base $B$:\[
b=b'\Longleftrightarrow M(b,B)=M(b',B)\]

\subsubsection*{Expresión matricial de una forma bilineal ante cambios de base. Matrices congruentes}

Dada otra base $B'$ de $V$, podemos considerar otra matriz $M(b,B')$. Supuesta también conocida $M(I_V,B\leftarrow B')=P$ y dados $\vec{u},\vec{v}\in V$ tales que:
\[\vec{u}=(a_1,\dots,a_n)_B=(a'_1,\dots,a'_n)_{B'}\]\[
\vec{v}=(b_1,\dots,b_n)_B=(b'_1,\dots,b'_n)_{B'}\]
Sabemos que:
\[
\begin{split}
b(\vec{u},\vec{v})=&\begin{pmatrix}
a_1 & \cdots & a_n
\end{pmatrix}M(b,B)\begin{pmatrix}
b_1 \\ \vdots \\ b_n
\end{pmatrix}\\
=&\begin{pmatrix}
a'_1 & \cdots & a'_n
\end{pmatrix}M(b,B')\begin{pmatrix}
b'_1 \\ \vdots \\ b'_n
\end{pmatrix}\\
\end{split}
\]
y que
\[
P\cdot\begin{pmatrix}
a_1'\\ \vdots \\ a_n'
\end{pmatrix}=\begin{pmatrix}
a_1\\ \vdots \\ a_n
\end{pmatrix}\hspace{2cm}P\cdot\begin{pmatrix}
b_1'\\ \vdots \\ b_n'
\end{pmatrix}=\begin{pmatrix}
b_1\\ \vdots \\ b_n
\end{pmatrix}
\]
Aplicando la expresión matricial a la ecuación anterior obtenemos:
\[
\begin{split}
b(\vec{u},\vec{v})=&\left(P\cdot\begin{pmatrix}
a_1'\\ \vdots \\ a_n'
\end{pmatrix}\right)^t \cdot M(b,B)\cdot P\cdot\begin{pmatrix}
b_1'\\ \vdots \\ b_n'
\end{pmatrix}\\
=&\begin{pmatrix}
a_1 & \cdots & a_n
\end{pmatrix}\cdot\underbrace{P^t \cdot M(b,B)\cdot P}_{\in\mathcal{M}_n(\mathbb{R})}\cdot\begin{pmatrix}
b_1'\\ \vdots \\ b_n'
\end{pmatrix}
\end{split}
\]

Si suponemos las coordenadas del vector $i$-ésimo de la base canónica para $\begin{pmatrix}a_1 & \cdots & a_n\end{pmatrix}$ y las coordenadas del vector $j$-ésimo de la base canónica para $\begin{pmatrix}b'_1 & \cdots & b'_n\end{pmatrix}^t$ obtenemos la coordenada $i,j$ de $P^t\cdot M(b,B)\cdot P$ que coincide con $M(b,B')$, es decir:
\begin{center}
\fcolorbox{black}{white}{$M(b,B')=P^t\cdot M(b,B)\cdot P$}
\end{center}
En esta situación, se dice que $A,B\in\mathcal{M}_n(\mathbb{R})$ son \textbf{congruentes} si existe una matriz $P\in GL(n,\mathbb{R})$ tal que $A=P^t\cdot B\cdot P$

La relación \textit{ser congruente a} es una \textbf{relación de equivalencia}:
\begin{itemize}
\item[$\bullet$] Toda matriz es semejante a sí misma.

Si tomamos $P=I_n$, tenemos $A=I_n^t\cdot A\cdot I_n\Leftrightarrow A=I_n\cdot A\cdot I_n\Leftrightarrow A=A$
\item[$\bullet$] Si $A$ es congruente a $B$, $B$ es congruente a $A$.

$A=P^t\cdot B\cdot P\Leftrightarrow (P^t)^{-1}\cdot A\cdot P^{-1}=B\Leftrightarrow (P^{-1})^t\cdot A\cdot P^{-1}=B$
\item[$\bullet$] Si $A$ es congruente a $B$ y $B$ es congruente a $C$, entonces $A$ es congruente a $C$.
\[
A=P^t\cdot B\cdot P\text{ y } B=Q^t\cdot C\cdot Q\Rightarrow A=P^t\cdot Q^t\cdot C\cdot Q\cdot P \Rightarrow A=(Q\cdot P)^t \cdot C\cdot (Q\cdot P)
\]
\end{itemize}

\textbf{Observación}

Dadas $A,B\in\mathcal{M}_n(\mathbb{R})$ dos matrices congruentes:
\begin{itemize}
\item[$\bullet$] $A$ y $B$ tienen el mismo rango.
\item[$\bullet$] $\det(A)$ y $\det(B)$ tienen el mismo signo.
\item[$\bullet$] $A$ es simétrica $\Leftrightarrow$ $B$ es simétrica.
\item[$\bullet$] $A$ es antisimétrica $\Leftrightarrow$ $B$ es antisimétrica.
\end{itemize}
Estas condiciones son necesarias pero no suficientes para comprobar que dos matrices son o no congruentes.

Por otro lado, \textit{ser congruente} y \textit{ser semejante} son dos relaciones independientes, por ejemplo:
\[\text{Las matrices }\begin{pmatrix}
1 & 1 & 0 & 0\\
0 & 2 & 0 & 0\\
0 & 0 & 3 & 0\\
0 & 0 & 0 & 4\\
\end{pmatrix} \text{ y }\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & 2 & 0 & 0\\
0 & 0 & 3 & 0\\
0 & 0 & 0 & 4\\
\end{pmatrix}\text{ son semejantes pero no congruentes.}\]

Las matrices\[
\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & 2 & 0 & 0\\
0 & 0 & 3 & 0\\
0 & 0 & 0 & 4\\
\end{pmatrix}\text{ y } \begin{pmatrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
\end{pmatrix}\]\(
\text{ son congruentes con }P=\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & \frac{1}{\sqrt{2}} & 0 & 0\\
0 & 0 & \frac{1}{\sqrt{3}} & 0\\
0 & 0 & 0 & \frac{1}{2}\\
\end{pmatrix}\) pero no son semejantes.

\subsection*{Estructura de Espacio Vectorial}

Dado un espacio vectorial $V(\mathbb{R})$ de dimensión $n$, consideramos $\mathbb{B}(V)$, que sabemos que es un conjunto no vacío porque al menos siempre contiene la aplicación nula.

Se define la siguiente aplicación:
\[\begin{split}
+:\mathbb{B}(V)\times\mathbb{B}(V)&\longrightarrow\mathbb{B}(V)\\
(b,b')&\longmapsto b+b'
\end{split}\]
La aplicación $b+b':V\times V\longrightarrow\mathbb{R}$ es una forma bilineal
\begin{align*}
 & \text{Definición: }(b+b')(\vec{u},\vec{v})=b(\vec{u},\vec{v})+(\vec{u},\vec{v})\hspace{2cm} \forall \vec{u},\vec{v}\in V \\
 & (b+b')(\lambda\vec{u}+\mu \vec{v},\vec{w})=b(\lambda\vec{u}+\mu \vec{v},\vec{w})+b'(\lambda\vec{u}+\mu \vec{v},\vec{w})=  \\
 & \lambda b(\vec{u},\vec{w})+\mu b(\vec{v},\vec{w}) + \lambda b'(\vec{u},\vec{w})+\mu b'(\vec{v},\vec{w})=\lambda(b+b')(\vec{u},\vec{w})+\mu (b+b')(\vec{v},\vec{w})  \\ 
  & \text{Análogamente: }\\
  & (b+b')(\vec{u},\lambda\vec{v}+\mu \vec{w})=\lambda (b+b')(\vec{u},\vec{v})+\mu(b+b')(\vec{u},\vec{w})\\
\end{align*}
Se puede comprobar que ($\mathcal{B}(V),+$) es un grupo abeliano.

Se define también la aplicación:
\[\begin{split}
\cdot :\mathbb{R}\times\mathcal{B}(V)&\longrightarrow\mathcal{B}(V)\\
(\lambda,b)&\longmapsto \lambda b\\
\end{split}\]
La aplicación $\lambda b:V\times V\longrightarrow\mathbb{R}$ es una forma bilineal.
\begin{align*}
 & \text{Definición: }(\lambda b)(\vec{u},\vec{v})=\lambda b(\vec{u},\vec{v})\\
 & (\lambda b)(\mu \vec{u}+\gamma\vec{v},\vec{w})=\lambda b(\mu \vec{u}+\gamma\vec{v},\vec{w})=\lambda\mu b(\vec{u},\vec{w})+\lambda\gamma b(\vec{v},\vec{w})=\\
 & \mu(\lambda b)(\vec{u},\vec{w})+\gamma(\lambda b)(\vec{v},\vec{w})\\
 & \text{Análogamente: }(\lambda b)(\vec{u},\mu \vec{v}+\gamma\vec{w})=\mu(\lambda b)(\vec{u},\vec{v})+\gamma(\lambda b)(\vec{u},\vec{w})\\
\end{align*}
$(\mathcal{B}(V),+,\cdot)$ tiene estructura de espacio vectorial.

\subsection*{Isomorfismo entre matrices cuadradas y formas bilineales. Formas bilineales simétricas y antisimétricas.}

Dado un e.v. $V(\mathbb{R})$ de dimensión $n\in\mathbb{N}$, consideramos su espacio de aplicaciones bilineales asociado $\mathcal{B}(V)$ y una base $B=\{\vec{v_1},\dots,\vec{v_n}\}$ de $V$. Definimos la siguiente aplicación:
\begin{align*}
F_B:\mathbb{B}(V)&\longrightarrow\mathcal{M}_n(\mathbb{R})\\
b & \longmapsto M(b,B)\\
\end{align*}
que demostraremos que es un isomorfismo de e.v., por lo que podremos afirmar que \fcolorbox{black}{white}{$\dim(\mathbb{B}(V))=n^2$}.
\newpage

\textbf{Demostración:}

Demostremos que $F_B$ es lineal:

$F_B$ es lineal $\Leftrightarrow$ $F_B(\lambda b+\mu b')=\lambda F_B(b)+\mu F_B(b') \hspace{10pt} \forall b,b'\in \mathcal{B}(V), \forall \lambda,\mu\in\mathbb{R}$

Por un lado:

$F_B(\lambda b+\mu b')=\underbrace{M(\lambda b+\mu b',B)}_{(a_{ij})}$

Por otro:

$\lambda F_B(b)+\mu F_B(b')=\lambda \underbrace{M(b,B)}_{(b_{ij})}+\mu \underbrace{M(b',B)}_{(c_{ij})}$

A su vez sabemos que para $1\leq i,j\leq n$ se cumple que:

\begin{align*}
 & a_{ij}=\underbrace{(\lambda b + \mu b')(\vec{v_i},\vec{v_j})=\lambda b(\vec{v_i},\vec{v_j})+\mu b'(\vec{v_i},\vec{v_j})}_{\text{ \scriptsize{Por la suma y el producto por escalares en }}\scriptstyle{\mathcal{B}(V)}} \\
 & b_{ij}=b(\vec{v_i},\vec{v_j}) \\
 & c_{ij}=b'(\vec{v_i},\vec{v_j}) \\
\end{align*}
Por tanto, $a_{ij}=\lambda b_{ij}+\mu c_{ij}$, luego $\mathbf{F_B}$ \textbf{es lineal}. Veamos que es inyectiva:
\[
\ker(F_B)=\{b\in \mathcal{B}(V)/F_B(b)=0\}=\{b\in \mathcal{B}(V)/M(b,B)=0\}=\{0\}
\]

Veamos que es sobreyectiva, es decir, que a cada matriz cuadrada se le puede asignar una forma bilineal.

Dada $A\in M_n(\mathbb{R}), \vec{u}=(a_1,\dots,a_n)_B, \vec{v}=(b_1,\dots,b_n)_B \in V$ consideramos la aplicación $b:V\times V\longrightarrow\mathbb{R}$ definida de la siguiente forma:
\[
b(\vec{u},\vec{v})=\begin{pmatrix}
a_1 & \cdots & a_n
\end{pmatrix}A\begin{pmatrix}
b_1 \\ \vdots \\ b_n
\end{pmatrix}
\]

La aplicación se demostró anteriormente que era bilineal.

Como $F_B$ es \textbf{lineal}, \textbf{inyectiva} y \textbf{sobreyectiva}, entonces es \textbf{biyectiva}, es decir, $\mathbf{F_B}$ \textbf{es un isomorfismo de e.v.}

\subsubsection*{Aplicación Traspuesta}
Dado un e.v. $V(\mathbb{R})$ de dimensión $n\in\mathbb{N}$, consideramos su espacio de aplicaciones bilineales asociado $\mathcal{B}(V)$. Definimos la siguiente aplicación:
\begin{align*}
T:\mathbb{B}(V)&\longrightarrow\mathbb{B}(V)\\
b&\longmapsto T(b)\\
\end{align*}
de forma que:
\begin{align*}
T(b):V\times V &\longrightarrow\mathbb{R}\\
(\vec{u},\vec{v})&\longmapsto T(b)(\vec{u},\vec{v})=b(\vec{v},\vec{u})\\
\end{align*}
La aplicación $T(b)$ está bien definida, es decir, $T(b)\in \mathcal{B}(V)$:

$\underbrace{T(b)(\lambda \vec{u}+\mu \vec{v},\vec{w})}=b(\vec{w},\lambda \vec{u}+\mu \vec{v})=\lambda b(\vec{w},\vec{u})+\mu b(\vec{w},\vec{v})=\underbrace{\lambda T(b)(\vec{u},\vec{w})+\mu T(b)(\vec{v},\vec{w})}$

Análogamente, $T(b)(\vec{u},\lambda\vec{v}+\mu\vec{w})=\lambda T(b)(\vec{u},\vec{v})+\mu T(b)(\vec{u},\vec{w})$

$\forall \vec{u},\vec{v},\vec{w}\in V, \forall \lambda,\mu\in\mathbb{R}$

Claramente, $T\circ T=T^2=I_{\mathcal{B}(V)}$, es decir, $T^2(b)=b\quad \forall b\in\mathcal{B}(V)$:
\[
T^2(b)(\vec{u},\vec{v})=T(T(b))(\vec{u},\vec{v})=T(b)(\vec{v},\vec{u})=b(\vec{u},\vec{v})\quad \forall \vec{u},\vec{v},\vec{w}\in V
\]

A continuación definimos dos subespacios vectoriales de $\mathcal{B}(V)$ muy particulares:
\[
\mathcal{B}_S(V)=\ker(T-I_{\mathcal{B}(V)})=\{b\in\mathcal{B}(V)/T(b)=b\}=\{b\in\mathcal{B}(V)/b(\vec{u},\vec{v})=b(\vec{v},\vec{u})\}\]\[
\mathcal{B}_A(V)=\ker(T+I_{\mathcal{B}(V)})=\{b\in\mathcal{B}(V)/T(b)=-b\}=\{b\in\mathcal{B}(V)/b(\vec{u},\vec{v})=-b(\vec{v},\vec{u})\}
\]\[\forall \vec{u},\vec{v}\in V\]

$\mathcal{B}_S(V)$ es el s.v. de las \textbf{formas bilineales simétricas} y $\mathcal{B}_A(V)$ es el s.v. de las \textbf{formas bilineales antisimétricas}.

Estos dos subespacios vectoriales cumplen una propiedad:
\[
\mathcal{B}(V)=\mathcal{B}_S(V)\oplus\mathcal{B}_A(V)
\]
Por lo que cada forma bilineal $b\in\mathcal{B}(V)$ se puede descomponer en suma de una única forma bilineal simétrica y una única forma bilineal antisimétrica, descompuestas de la siguiente forma:
\[
b=\underbrace{\frac{1}{2}(b+T(b))}_{\in \mathcal{B}_S(V)}+\underbrace{\frac{1}{2}(b-T(b))}_{\in \mathcal{B}_A(V)}
\]
Como observación, las formas bilineales antisimétricas cumplen que $\mathcal{B}_A(V)=\{b\in\mathcal{B}(V)/b(\vec{u},\vec{u})=0,\quad \forall \vec{u}\in V\}$

\textbf{Demostración}

$\subseteq)$
\[
b(\vec{u},\vec{v})=-b(\vec{v},\vec{u})\quad\forall \vec{u},\vec{v}\in V\]\[
\text{Para }\vec{u}=\vec{v}\]\[ b(\vec{u},\vec{u})=-b(\vec{u},\vec{u})\Rightarrow b(\vec{u},\vec{u})=0\quad\forall \vec{u}\in V\]


$\supseteq)$

Dados $\vec{u},\vec{v}\in V$
\[\underbrace{0}=b(\vec{u}+\vec{v},\vec{u}+\vec{v})=\overbrace{(\vec{u},\vec{u})}^{=0}+b(\vec{u},\vec{v})+b(\vec{v},\vec{u})+\overbrace{b(\vec{v},\vec{v})}^{=0}=\underbrace{b(\vec{u},\vec{v})+b(\vec{v},\vec{u})}\]

Si fijamos una base $B=\{\vec{v_1},\dots,\vec{v_n}\}$ y tomamos de nuevo el isomorfismo $F_B$ podemos establecer la siguiente proposición:

\textbf{Proposición:}

Dado un e.v. $V(\mathbb{R})$ de dimensión $n\in\mathbb{N}$, consideramos su espacio de aplicaciones bilineales asociado $\mathcal{B}(V)$ y una base $B=\{\vec{v_1},\dots,\vec{v_n}\}$ de $V$. Considerando el isomorfismo $F_B$ se cumple que:
\begin{enumerate}
\item $F_B(\mathcal{B}_S(V))=S_n(\mathbb{R});\quad \dim(\mathcal{B}_S(V))=\frac{n(n+1)}{2}$
\item $F_B(\mathcal{B}_A(V))=A_n(\mathbb{R});\quad \dim(\mathcal{B}_A(V))=\frac{n(n-1)}{2}$
\end{enumerate}

\textbf{Demostración:}

Para demostrarlo, enunciaremos la siguiente proposición:

Dada una forma bilineal $b\in\mathcal{B}(V)$. Las siguientes afirmaciones son equivalentes:

\begin{enumerate}
\item $b\in\mathcal{B}_S(V)$
\item Para toda base $B$ de $V$, $M(b,B)\in S_n(\mathbb{R})$.
\item Existe una base $B$ de $V$ tal que $M(b,B)\in S_n(\mathbb{R})$.
\end{enumerate}
\newpage
\textbf{Demostración:}

1)$\Rightarrow$2)
Sea $B=\{\vec{v_1},\dots,\vec{v_n}\}$ base de $V$

$M(b,B)=(a_{ij})_{1\leq i,j \leq n}\in S_n(\mathbb{R})\Leftrightarrow a_{ij}=a_{ji} \Leftrightarrow b(\vec{v_i},\vec{v_j})=b(\vec{v_j},\vec{v_i})$

$\forall i,j\in\{1,\dots,n\}$ Esta última equivalencia se cumple por ser $b$ simétrica por hipótesis.

2)$\Rightarrow$3) Trivial

3)$\Rightarrow$1)

Por hipótesis, existe una base $B$ de $V$ tal que $M(b,B)\in S_n(\mathbb{R})$. 

Consideramos $\vec{u}=(a_1,\dots,a_n)_B, \vec{v}=(b_1,\dots,b_n)_B\in V$

Si aplicamos b a $\vec{u}$ y $\vec{v}$:

\[b(\vec{u},\vec{v})=\begin{pmatrix}
a_1 & \cdots & a_n
\end{pmatrix}M(b,B)\begin{pmatrix}
b_1 \\ \vdots \\ b_n
\end{pmatrix}\]
\[b(\vec{v},\vec{u})=\begin{pmatrix}
b_1 & \cdots & b_n
\end{pmatrix}M(b,B)\begin{pmatrix}
a_1 \\ \vdots \\ a_n
\end{pmatrix}\]

Como $b(\vec{u},\vec{v})\in\mathbb{R}$, $b(\vec{u},\vec{v})=b(\vec{u},\vec{v})^t$, es decir:

\[b(\vec{u},\vec{v})=b(\vec{u},\vec{v})^t=\left(\begin{pmatrix}
a_1 & \cdots & a_n
\end{pmatrix}M(b,B)\begin{pmatrix}
b_1 \\ \vdots \\ b_n
\end{pmatrix}\right)^t=\]\[\begin{pmatrix}
b_1 & \cdots & b_n
\end{pmatrix}M(b,B)^t\begin{pmatrix}
a_1 \\ \vdots \\ a_n
\end{pmatrix}\shortstack{{\tiny{por hipótesis}}\\=}\begin{pmatrix}
b_1 & \cdots & b_n
\end{pmatrix}M(b,B)\begin{pmatrix}
a_1 \\ \vdots \\ a_n
\end{pmatrix}=\]\[
b(\vec{v},\vec{u})\quad\forall\vec{u},\vec{v}\in V\]

Análogamente, para las formas bilineales antisimétricas existe una proposición similar:
\newpage
Dada una forma bilineal $b\in\mathcal{B}(V)$. Las siguientes afirmaciones son equivalentes:

\begin{enumerate}
\item $b\in\mathcal{B}_A(V)$
\item Para toda base $B$ de $V$, $M(b,B)\in A_n(\mathbb{R})$.
\item Existe una base $B$ de $V$ tal que $M(b,B)\in A_n(\mathbb{R})$.
\end{enumerate}

\textbf{Demostración:}

1)$\Rightarrow$2)
Sea $B=\{\vec{v_1},\dots,\vec{v_n}\}$ base de $V$

$M(b,B)=(a_{ij})_{1\leq i,j \leq n}\in A_n(\mathbb{R})\Leftrightarrow a_{ij}=-a_{ji} \Leftrightarrow b(\vec{v_i},\vec{v_j})=-b(\vec{v_j},\vec{v_i})$

$\forall i,j\in\{1,\dots,n\}$ Esta última equivalencia se cumple por ser $b$ antisimétrica por hipótesis.

2)$\Rightarrow$3) Trivial

3)$\Rightarrow$1)

Por hipótesis, existe una base $B$ de $V$ tal que $M(b,B)\in A_n(\mathbb{R})$. 

Consideramos $\vec{u}=(a_1,\dots,a_n)_B, \vec{v}=(b_1,\dots,b_n)_B\in V$

Si aplicamos b a $\vec{u}$ y $\vec{v}$:

\[b(\vec{u},\vec{v})=\begin{pmatrix}
a_1 & \cdots & a_n
\end{pmatrix}M(b,B)\begin{pmatrix}
b_1 \\ \vdots \\ b_n
\end{pmatrix}\]
\[b(\vec{v},\vec{u})=\begin{pmatrix}
b_1 & \cdots & b_n
\end{pmatrix}M(b,B)\begin{pmatrix}
a_1 \\ \vdots \\ a_n
\end{pmatrix}\]

Como $b(\vec{u},\vec{v})\in\mathbb{R}$, $b(\vec{u},\vec{v})=b(\vec{u},\vec{v})^t$, es decir:

\[b(\vec{u},\vec{v})=b(\vec{u},\vec{v})^t=\left(\begin{pmatrix}
a_1 & \cdots & a_n
\end{pmatrix}M(b,B)\begin{pmatrix}
b_1 \\ \vdots \\ b_n
\end{pmatrix}\right)^t=\]\[\begin{pmatrix}
b_1 & \cdots & b_n
\end{pmatrix}M(b,B)^t\begin{pmatrix}
a_1 \\ \vdots \\ a_n
\end{pmatrix}\shortstack{{\tiny{por hipótesis}}\\=}-\begin{pmatrix}
b_1 & \cdots & b_n
\end{pmatrix}M(b,B)\begin{pmatrix}
a_1 \\ \vdots \\ a_n
\end{pmatrix}=\]\[
-b(\vec{v},\vec{u})\quad\forall\vec{u},\vec{v}\in V\]

\textbf{FALTA LA DEMOSTRACIÓN PROPIAMENTE DICHA}

\subsection*{Producto tensorial}

Dado un e.v. $V(\mathbb{R})$ de dimensión $n\in\mathbb{N}$, se considera una base $B=\{\vec{v_1},\dots,\vec{v_n}\}$ de $V$ y su base dual $B^*=\{\varphi_1,\dots,\varphi_n\}$. Tomamos dos formas lineales, $\varphi,\psi\in V^*$, se define la siguiente aplicación:
\begin{align*}
\varphi\otimes\psi: V\times V & \longrightarrow\mathbb{R}\\
(\vec{u},\vec{v})&\longmapsto\varphi\otimes\psi(\vec{u},\vec{v})=\varphi(\vec{u})\cdot\psi(\vec{v})\\
\forall \vec{u},\vec{v}\in V&
\end{align*}
conocida como \textbf{producto tensorial de }$\mathbf{\varphi}$\textbf{ y }$\mathbf{\psi}$.

\textbf{Propiedades:}
\begin{enumerate}
\item $\varphi\otimes\psi\in\mathcal{B}(V)\quad\forall\varphi,\psi\in V^*$
\item $\varphi\otimes\psi=0\Longleftrightarrow\varphi=0 \vee \psi=0$
\item \((\lambda\cdot\varphi+\mu\cdot\psi)\otimes\eta=\lambda(\varphi\otimes\eta)+\mu(\psi\otimes\eta)\\\)\(
		\varphi\otimes(\lambda\cdot\psi+\mu\cdot\eta)=\lambda(\varphi\otimes\psi)+\mu(\varphi\otimes\eta)\\ \)\(\forall \varphi,\psi,\eta\in V^*,\ \forall \lambda,\mu\in \mathbb{R}\)
\item $\varphi\otimes\psi\in\mathcal{B}_S(V)\Longleftrightarrow\{\varphi,\psi\}\text{ es l.d.}$
\item $\varphi\otimes\psi\in\mathcal{B}_A(V)\Longleftrightarrow\varphi\otimes\psi=0$
\item Si $\varphi=(a_1,\dots,a_n)_{B^*}$ y $\psi=(b_1,\dots,b_n)_{B^*}$, entonces:
\[
M(\varphi\otimes\psi,B)=\begin{pmatrix}
a_1b_1 & a_1b_2 & \cdots & a_1b_n\\
a_2b_1 & a_2b_2 & \cdots & a_2b_n\\
\vdots & \vdots & \ddots & \vdots\\
a_nb_1 & a_nb_2 & \cdots & a_nb_n\\
\end{pmatrix}=\begin{pmatrix}
a_1 \\ a_2 \\ \vdots \\ a_n
\end{pmatrix}_{n\times 1}\begin{pmatrix}
b_1 & b_2 & \cdots & b_n\\
\end{pmatrix}_{1\times n}
\]
En particular, $M(\varphi_i\otimes\varphi_j,B)=E_{ij}$.
\end{enumerate}

\textbf{Demostración}
\begin{enumerate}
\item 
\[(\varphi\otimes\psi)(\lambda\vec{u}+\mu\vec{v},\vec{w})=\varphi(\lambda\vec{u}+\mu\vec{v})\psi(\vec{w})\shortstack{{\tiny{$\varphi\in V^*$}}\\=}[\lambda\cdot\varphi(\vec{u})+\mu\cdot\varphi(\vec{v})]\psi(\vec{w})=\]
\[\lambda\cdot\varphi(\vec{v})\psi(\vec{w})+\mu\cdot\varphi(\vec{v})\psi(\vec{w})=\lambda(\varphi\otimes\psi)(\vec{u},\vec{w})+\mu(\varphi\otimes\psi)(\vec{v},\vec{w})\]
Análogamente, $(\varphi\otimes\psi)(\vec{u},\lambda\vec{v}+\mu \vec{w})=\lambda(\varphi\otimes\psi)(\vec{u},\vec{v})+\mu(\varphi\otimes\psi)(\vec{u},\vec{w})$
\(\forall \phi,\psi\in V^*,\quad\forall\vec{u},\vec{v},\vec{w}\in V,\quad \forall\lambda\mu\in\mathbb{R}\)
\item
\(\varphi\otimes\psi=0\Leftrightarrow(\varphi\otimes\psi)(\vec{u},\vec{v})=0\Leftrightarrow\varphi(\vec{u})\psi(\vec{v})=0\quad \forall \vec{u},\vec{v}\in V\Leftrightarrow\\\)
\(\varphi=0\text{ ó }\psi=0\)
\item Evaluemos $(\lambda\cdot\varphi+\mu\cdot\psi)\otimes\eta$ en una pareja de vectores cualesquiera:

\(((\lambda\cdot\varphi+\mu\cdot\psi)\otimes\eta)(\vec{u},\vec{v})=(\lambda\cdot\varphi+\mu\cdot\psi)(\vec{u})\cdot\eta(\vec{v})=(\lambda\cdot\varphi(\vec{u})+\mu\cdot\psi(\vec{u}))\cdot\eta(\vec{v})=\\\)
\(\lambda\cdot\varphi(\vec{u})\eta(\vec{v})+\mu\cdot\psi(\vec{u})\eta(\vec{v})=\lambda(\varphi\otimes\eta)(\vec{u},\vec{v})+\mu (\psi\otimes\eta)(\vec{u},\vec{v})\)

La segunda propiedad se demuestra de una manera similar.
\item
Para demostrar esta propiedad veamos primero que ocurre si se le aplica la aplicación $T$ a un producto tensorial de dos formas lineales, para ello, evaluemos el producto tensorial $\varphi\otimes\psi$ en una pareja de vectores cualesquiera $\vec{u},\vec{v}\in V$:

\(T(\varphi\otimes\psi)(\vec{u},\vec{v})=(\varphi\otimes\psi)(\vec{v},\vec{u})=\varphi(\vec{v})\psi(\vec{u})=\psi(\vec{u})\varphi(\vec{v})=(\psi\otimes\varphi)(\vec{u},\vec{v})\)

Es decir, $T(\varphi\otimes\psi)=\psi\otimes\varphi$. Por lo que al descomponer $\varphi\otimes\psi$ como suma de una aplicación simétrica y otra antisimétrica, se tiene:

\[
\varphi\otimes\psi=\underbrace{\frac{1}{2}(\varphi\otimes\psi+\psi\otimes\varphi)}_{\in\mathcal{B}_S(V)}+\underbrace{\frac{1}{2}(\varphi\otimes\psi-\psi\otimes\varphi)}_{\in\mathcal{B}_A(V)}
\]

\begin{itemize}

\item[$\Leftarrow$]

Si $\varphi=0$ o $\psi=0$ $\Rightarrow$ $\varphi\otimes\psi=0\Rightarrow\varphi\otimes\psi\in\mathcal{B}_S(V)$.

Si $\varphi\not= 0$ y $\psi\not= 0$, entonces, por ser $\{\varphi,\psi\}$ linealmente dependiente, existe un escalar $\lambda\in\mathbb{R}$ tal que $\varphi=\lambda\cdot\psi$. Teniendo esto en cuenta:

\[\varphi\otimes\psi=(\lambda\cdot\psi)\otimes\psi=\lambda\cdot(\psi\otimes\psi)\]
Al aplicarle la aplicación $T$:
\[T(\varphi\otimes\psi)=T(\lambda\cdot(\psi\otimes\psi))=\lambda\cdot T(\psi\otimes\psi)=\lambda\cdot(\psi\otimes\psi)=\varphi\otimes\psi\]
Luego $\varphi\otimes\psi\in\mathcal{B}_S(V)$

\item[$\Rightarrow$]

Si $\varphi\otimes\psi\in\mathcal{B}_S(V)$, entonces $T(\varphi\otimes\psi)=\varphi\otimes\psi$, o equivalentemente: $\psi\otimes\varphi=\varphi\otimes\psi$. Al evaluar la aplicación en una pareja de vectores cualesquiera $\vec{u},\vec{v}\in V$, se tiene que:
\[(\varphi\otimes\psi)(\vec{u},\vec{v})=(\psi\otimes\varphi)(\vec{u},\vec{v})\Leftrightarrow\varphi(\vec{u})\psi(\vec{v})=\psi(\vec{u})\varphi(\vec{v})\quad\forall\vec{u},\vec{v}\in V\]
Si fijamos uno de los dos vectores, por ejemplo, $\vec{u}=\vec{u_0}$, y lo pasamos todo a un solo miembro:
\[\varphi(\vec{u_0})\psi(\vec{v})-\psi(\vec{u_0})\varphi(\vec{v})=0\Leftrightarrow(\varphi(\vec{u_0})\psi-\psi(\vec{u_0})\varphi)(\vec{v})=0\quad\forall\vec{v}\in V\]

Al ser válido para cualquier vector de $\vec{v}\in V$, entonces $(\varphi(\vec{u_0})\psi-\psi(\vec{u_0})\varphi)=0$, como a su vez, es válido para cualquier vector $\vec{u_0}\in V$, no solo el vector nulo, se tiene una combinación lineal de $\{\varphi,\psi\}$ cuyos escalares son no todos nulos igualada a cero, es decir, $\{\varphi,\psi\}$ es linealmente dependiente.

\end{itemize}
\item

\begin{itemize}
\item[$\Rightarrow$]

Si $\varphi\otimes\psi\in\mathcal{B}_A(V)$, entonces  $T(\varphi\otimes\psi)=-\varphi\otimes\psi$, o equivalentemente: $\psi\otimes\varphi=-\varphi\otimes\psi$. Al evaluar la aplicación en una pareja de vectores cualesquiera $\vec{u},\vec{v}\in V$, se tiene que:
\[(\varphi\otimes\psi)(\vec{u},\vec{v})=-(\psi\otimes\varphi)(\vec{u},\vec{v})\Leftrightarrow\varphi(\vec{u})\psi(\vec{v})=-\psi(\vec{u})\varphi(\vec{v})\quad\forall\vec{u},\vec{v}\in V\]
De donde se obtiene que  $(\varphi(\vec{u})\psi+\psi(\vec{u})\varphi)=0$ por el mismo procedimiento de la demostración de 4). Obtenemos una combinación lineal de $\{\varphi,\psi\}$ cuyos escalares son no todos nulos igualada a cero, es decir, $\{\varphi,\psi\}$ es linealmente dependiente, que por la propiedad 4) sabemos que entonces $\varphi\otimes\psi\in\mathcal{B}_S(V)$, y como $\varphi\otimes\psi\in\mathcal{B}_S(V)\cap\mathcal{B}_A(V)$, entonces $\varphi\otimes\psi=0$.
\item[$\Leftarrow$] Trivialmente $0\in\mathcal{B}_A(V)$.

\end{itemize}
\item

Dadas dos formas lineales $\varphi,\psi\in V^*$ cuyas coordenadas en la base $B^*=\{\varphi_1,\dots,\varphi_n\}$, base dual asociada a la base $B=\{\vec{v_1},\dots,\vec{v_n}\}$ de $V$ son: $\varphi=(a_1,\dots,a_n)_{B^*}$ y $\psi=(b_1,\dots,b_n)_{B^*}$, nos preguntamos cual es la matriz $M(\varphi\otimes\psi, B)=(a_{ij})$
\[a_{ij}=(\varphi\otimes\psi)(\vec{v_i},\vec{v_j})=\varphi(\vec{v_i})\psi(\vec{v_j})=a_ib_j\quad\forall i,j\in\{1,\dots,n\}\]
\end{enumerate}


De esta última propiedad, y retomando el isomorfismo $F_B$ fijada una base $B$, pero en este caso la aplicación inversa $F_B^{-1}$, consideramos que para $i,j\in\{1,\dots,n\}$, las matrices $E_{ij}$ forman una base de $\mathcal{M}_n(\mathbb{R})$. De esta forma, aplicando el isomorfismo de tal forma que $F_B^{-1}(E_{ij})=\varphi_i\otimes\varphi_j$, podemos concluir que \fcolorbox{black}{white}{$\mathbf{\{\varphi_i\otimes\varphi_j/1\leq i,j\leq n\}}$ \textbf{es una base de $\mathcal{B}(V)$}}.

Sabiendo esto, construyamos una forma bilineal $b$ a partir de esta base.

Si $M(b,B)=\sum_{i,j=1}^n b(\vec{v_i},\vec{v_j})\cdot E_{ij}$.

Si aplicamos el isomorfismo de forma inversa:
\[F_B^{-1}(M(b,B))=b=\sum_{i,j=1}^n b(\vec{v_i},\vec{v_j})\cdot \varphi_i\otimes\varphi_j\]

En particular, para el caso de matrices cuadradas / formas bilineales simétricas y antisimétricas, se tiene que:
\[B_{S_n(\mathbb{R})}=\{E_{ii}/1\leq i\leq n\}\cup\{E_{ij}+E_{ji}/1\leq i,j \leq n\}\]
\[B_{A_n(\mathbb{R})}=\{E_{ij}-E_{ji}/1\leq i,j \leq n\}\]
y
\[B_{\mathcal{B}_S(V)}=\{\varphi_i\otimes\varphi_i/1\leq i\leq n\}\cup\{\varphi_i\otimes\varphi_j+\varphi_j\otimes\varphi_i/1\leq i,j \leq n\}\]
\[B_{\mathcal{B}_A(V)}=\{\varphi_i\otimes\varphi_j-\varphi_j\otimes\varphi_i/1\leq i,j \leq n\}\]

son, respectivamente, bases de $S_n(\mathbb{R})$, $A_n(\mathbb{R})$, $\mathcal{B}_S(V)$ y $\mathcal{B}_A(V)$.


\newpage
\section{Clasificación de métricas y formas cuadráticas reales}

Dado un espacio vectorial real $V(\mathbb{R})$, se consideramos una forma bilineal simétrica $g\in\mathcal{B}_S(V)$, a la que a partir de ahora también llamaremos \textbf{métrica}. Fijada esta métrica, a la dupla $(V,g)$ se denota como \textbf{espacio vectorial métrico}. Asociado a este, existe una \textbf{forma cuadrática}, término que definimos a continuación:

\textbf{Definición}

Dado un espacio vectorial métrico $(V,g)$, asociado a este se considera una aplicación:
\[\begin{split}
\omega_g:V&\longrightarrow\mathbb{R}\\
\vec{v}&\longmapsto\omega_g(\vec{v})=g(\vec{v},\vec{v})
\end{split}\]
A esta aplicación se le llama \textbf{forma cuadrática asociada a la métrica $g$}.
Una forma cuadrática $\omega$ ha de cumplir las siguientes propiedades:
\begin{itemize}
\item $\omega(\lambda\vec{v})=\lambda^2\omega(\vec{v})\quad\forall\vec{v}\in V, \forall\lambda\in\mathbb{R}$
\item La aplicación $g_\omega:V\rightarrow\mathbb{R},\quad g_\omega(\vec{u},\vec{v})=\frac{1}{2}(\omega(\vec{u}+\vec{v})-\omega(\vec{u})-\omega(\vec{v})$ es una métrica. Y a esta se le conoce como la \textbf{métrica asociada a la forma cuadrática }$\mathbf{\omega}$.
\end{itemize}

Así, se puede observar que a cada métrica $g$ se le puede asociar una forma cuadrática $\omega_g$ y que a cada forma cuadrática $\omega$ se le puede asociar una métrica $g_\omega$, es decir, existe una biyección entre métricas y formas cuadráticas.

Se denota $\mathcal{F}(V)=\{\omega:V\longrightarrow\mathbb{R}/\omega \text{ es una forma cuadrática sobre V }\}$ y  dadas $\omega,\omega'\in\mathcal{F}(V)$ y $\lambda\in\mathbb{R}$ se definen las operaciones:
\begin{align*}
\omega+\omega':V&\longrightarrow\mathbb{R} & (\omega+\omega')(\vec{v})&=\omega(\vec{v})+\omega'(\vec{v})\\
\lambda\omega:V&\longrightarrow\mathbb{R} & (\lambda\omega)(\vec{v})&=\lambda\omega(\vec{v})\\
\end{align*}

$\mathcal{F}(V)$, con las operaciones de suma y producto por escalares definidas anteriormente tiene estructura de espacio vectorial, la demostración se deja como ejercicio.

Ya definido $\mathcal{F}(V)$ y habiendo comprobado que existe una biyección entre métricas ($\mathcal{B}_S(V)$) y formas cuadráticas($\mathcal{F}(V)$) es hora de definir la aplicación 
\[\begin{split}
F:\mathcal{F}(V)&\longrightarrow\mathcal{B}_S(V)\\
\omega&\longmapsto g_\omega\\
\end{split}\]
que es un isomorfismo de e.v.
\textbf{Demostración}
Es un pestiño que añadiré cuando termine el curso xd.

\subsection{Clasificación de métricas}
Antes de clasificar las métricas, se introducen unas nociones sobre ortogonalidad y perpendicularidad:

Dado $(V,g)$ un e.v. métrico y dados $\vec{u},\vec{v}\in V$, se dice que $\vec{u}$\textit{ es perpendicular a }$\vec{v}$, que $\vec{u}$ y $\vec{v}$ son perpendiculares u ortogonales, y se denota como $\vec{u}\perp\vec{v}$ si, y solo si, por definición, $g(\vec{u},\vec{v})=0$, equivalentemente, $\omega_g(\vec{u}+\vec{v})=\omega_g(\vec{u})+\omega_g(\vec{v})$.

Sea $U$ un s.v. de $V$, se dice que un vector $\vec{v}\in V$ es perpendicular a $U$ ($\vec{u}\perp U$) si, y solo si, $\vec{v}\perp\vec{u}\quad\forall\vec{u}\in U$.

Sean $U$ y $W$ dos s.v. de $V$, se dice que $U$ y $W$ son perpendiculares ($U\perp W$) si, y solo si $\vec{u}\perp\vec{w}\quad\forall\vec{u}\in U,  \ \forall\vec{w}\in W$.

Sean $U_1, U_2,\dots ,U_k$ $k$ subespacios vectoriales de $V$, se dice que $V$ es \textbf{suma ortogonal} de $U_1,\dots ,U_k$, y se denota $V=U_1\obot U_2\obot\cdots\obot U_k$ si, y solo si, se cumplen las siguientes condiciones:
\begin{itemize}
\item $V=U_1\oplus U_2\oplus\cdots\oplus U_k$
\item $U_i\perp U_j \quad\forall i,j\in\{1,\dots ,k\} \ i\not=j$
\end{itemize}

\textbf{Observación}
\begin{itemize}
\item $\vec{u}\perp U\Longleftrightarrow\vec{u}\perp\vec{v_i}\quad i\in\{1,\dots,k\}\text{ si }U=L(\{\vec{v_1},\dots,\vec{v_k}\})$.
\item Dados $U,W$ s.v. de $V$ tales que $U=L(\{\vec{u_1},\dots,\vec{u_k}\})$ y $W=L(\{\vec{w_1},\dots,\vec{w_p}\})$, entonces:

$U\perp W\Longleftrightarrow\vec{u_i}\perp\vec{w_j}\quad\forall i\in\{1,\dots,k\}, \ \forall j\in\{1,\dots,p\}$
\end{itemize}

Una vez introducido este concepto, dado $V(\mathbb{R})$ un e.v. real y $g\in\mathcal{B}_S(V)$ una métrica no nula. Distinguimos entre:
\begin{itemize}
\item $g$ es \textbf{degenerada} si: $\exists\vec{v}\in V, v\not=0$ tal que $\vec{v}\perp\vec{u} \quad \forall\vec{u}\in V$. Estas se subdividen en:
\begin{itemize}
\item $g$ es \textbf{semidefinida positiva} si $\omega_g(\vec{v})\geq 0\quad\forall\vec{v}\in V$ y $\exists\vec{v_0}\in V, \vec{v_0}\not= 0$ tal que $\omega(\vec{v_0})=0$ (equivalentemente, $\omega_g(V-\{0\})=\mathbb{R}_0^+$).
\item $g$ es \textbf{semidefinida negativa} si $\omega_g(\vec{v})\leq 0\quad\forall\vec{v}\in V$ y $\exists\vec{v_0}\in V, \vec{v_0}\not= 0$ tal que $\omega(\vec{v_0})=0$ (equivalentemente, $\omega_g(V-\{0\})=\mathbb{R}_0^-$).
\item $g$ es \textbf{indefinida} si $\exists \vec{u},\vec{v}\in V$ tales que $\omega_g(\vec{u})>0$ y $\omega_g(\vec{v})<0$ (equivalentemente, $\im(\omega_g)=\mathbb{R}$).
\end{itemize}
\item $g$ es \textbf{no degenerada} si verifica que si $\vec{v}\in V$ y $\vec{v}\perp\vec{u}\quad\forall\vec{u}\in V$, entonces $\vec{v}=0$, es decir, el único vector de $V$ perpendicular a todos los vectores de $V$ es el vector nulo. Estas se subdividen en:
\begin{itemize}
\item $g$ es \textbf{definida positiva} o \textbf{euclídea} si $\omega_g(\vec{v})> 0\quad\forall\vec{v}\in V-\{0\}$ (equivalentemente, $\omega_g(V-\{0\})=\mathbb{R}^+$).
\item $g$ es \textbf{definida negativa} si $\omega_g(\vec{v})<0\quad\forall\vec{v}\in V-\{0\}$ (equivalentemente, $\omega_g(V-\{0\})=\mathbb{R}^-$).
\item $g$ es \textbf{indefinida} si $\exists \vec{u},\vec{v}\in V$ tales que $\omega_g(\vec{u})>0$ y $\omega_g(\vec{v})<0$ (equivalentemente, $\im(\omega_g)=\mathbb{R}$).
\end{itemize}
\end{itemize}

\textbf{Observación}

$g$ es semidefinida positiva (resp. semidefinida negativa) y $\vec{v_0}\in V\vec{v_0}\not=0$
$\vec{v_0}\perp\vec{v}\quad\forall\vec{v}\in V\Longleftrightarrow\omega_g(\vec{v_0})=0$


\textbf{Demostración}

$\Rightarrow)$ Trivialmente si $\vec{v_0}\perp\vec{v}\quad\forall\vec{v}\in V$, entonces, en concreto $vec{v_0}\perp\vec{v_0}$, luego $g(\vec{v_0},\vec{v_0})=\omega_g(\vec{v_0})=0$.

$\Leftarrow)$ Dado $\vec{v}\in V$, sea $f:\mathbb{R}\longrightarrow\mathbb{R}$ la función definida como 
\begin{align*}
f(t)&=\omega_g(\vec{v}+t\vec{v_0})\\
&=g(\vec{v}+t\vec{v_0},\vec{v}+t\vec{v_0})\\
&=\omega(\vec{v})+\underbrace{t^2\omega_g(\vec{v_0})}_{=0}+2tg(\vec{v},\vec{v_0})\\
&=\omega_g(\vec{v})+2tg(\vec{v},\vec{v_0})\\
\end{align*}

En caso de que $f(t)>0\quad\forall t\in\mathbb{R}$, ($g$ es semidefinida positiva).

En caso de que $f(t)<0\quad\forall t\in\mathbb{R}$, ($g$ es semidefinida negativa).

\subsection{Radical de la métrica}

Dado $(V,g)$ un espacio vectorial métrico de dimensión $n$, consideramos una base $B=\{\vec{v_1},\dots,\vec{v_n}$ de $V$.

Se define el \textbf{Radical de g} como el conjunto:
\[\rad(g)=\{\vec{v}\in V/\vec{v}\perp\vec{u},\quad\forall\vec{u}\in V\}\]
Es decir, el conjunto de vectores perpendiculares a todos los de $V$.

\textbf{Observación}

$\rad(g)$ es un subespacio vectorial.

\textbf{Demostración}

Sean $\lambda,\mu\in\mathbb{R}$ y sean $\vec{u},\vec{v}\in\rad(g),\vec{w}\in V$.
\[g(\lambda\vec{u}+\mu \vec{v},\vec{w})=\lambda g(\vec{u},\vec{w})+\mu g(\vec{v},\vec{w})\]
Como $\vec{u},\vec{v}\in\rad(g)$, entonces $g(\vec{u},\vec{w})=g(\vec{v},\vec{w})=0$, luego $g(\lambda\vec{u}+\mu \vec{v},\vec{w})=0$, por lo que $\lambda\vec{u}+\mu \vec{v}\in\rad(g)$.

A continuación se definen algunos conceptos importantes relacionados con el radical y su dimensión:
\begin{itemize}
\item Se define la \textbf{nulidad} de una métrica $g$ como la dimensión del radical de $g$ (nulidad($g$)=$\dim(\rad(g))$).
\item Se define el \textbf{rango} de una métrica $g$ como la diferenci entre la dimensión de $V$ y la nulidad de $g$ (rango($g$)=$n-$nulidad($g$)).
\end{itemize}

\textbf{Procedimiento para hallar el radical de una métrica:}

Dados $\vec{v_0}\in \rad(g),\vec{v}\in V$, se consideran sus coordenadas en la base $B$:
\[\vec{v_0}=(a_1,a_2,\dots,a_n)_B\hspace{2cm}\vec{v}=(x_1,x_2,\dots,x_n)_B\]
\begin{align*}
\vec{v_0}\in\rad (g) & \Leftrightarrow \vec{v_0}\perp\vec{v}\\
&\Leftrightarrow g(\vec{v_0},\vec{v})=0\\
&\Leftrightarrow\begin{pmatrix}
x_1 & x_2 & \cdots & x_n
\end{pmatrix} M(g,B) \begin{pmatrix}
a_1 \\ a_2 \\ \vdots \\ a_n
\end{pmatrix}=0\quad\forall (x_1,x_2,\dots,x_n)\in\mathbb{R}^n\\
&\Leftrightarrow M(g,B) \begin{pmatrix}
a_1 \\ a_2 \\ \vdots \\ a_n
\end{pmatrix}=\begin{pmatrix}
0 \\ 0 \\ \vdots \\ 0
\end{pmatrix}
\end{align*}
$\rad(g)=\lbrace \vec{v}\in V, \vec{v}=(a_1,a_2,\dots,a_n)_B / M(g,b)\begin{pmatrix}
a_1\\ a_2 \\ \vdots \\ a_n
\end{pmatrix}=\begin{pmatrix}
0 \\ 0 \\ \vdots \\ 0
\end{pmatrix}\rbrace\cong\ker(M(g,B))$
De esta forma, calcular el radical es equivalente a calcular el núcleo de la matriz de la métrica en cualquier base.

Después de este razonamiento podemos concluir una serie de igualdades de interés:
\[\text{nulidad}(g)=\dim(\rad(g))=n-\text{rango}(M(g,B))=n-\text{rango}(g)\]
Y además, $\text{rango}(g)=\text{rango}(M(g,B))$, es decir, el rango y la nulidad de una métrica no depende de la base.
\subsection{Ejemplos}
El siguiente ejemplo será muy útil a la hora de clasificar métricas.
En $\mathbb{R}^n$:

Sea $A\in\mathcal{M}_n(\mathbb{R})$. Con anterioridad se demostró que la forma bilineal asociada a esta matriz cumplía que $b_A\in\mathcal{B}_S(\mathbb{R}^n)\Leftrightarrow A\in\mathcal{S}_n(\mathbb{R})$. En este caso, por ser una métrica la llamaremos $g_A$.

Recordemos: $g_A(\vec{u},\vec{v})=\vec{u}A\vec{v}$ y $\omega_A(\vec{v})=\vec{v}A\vec{v}\quad\forall\vec{u},\vec{v}\in \mathbb{R}^n$.

Clasifiquemos $g_A$ para una matriz $A$ particular:
A=$\left(\begin{array}{c|c|c}
I_p & 0 & 0 \\ \hline
0 & -I_q & 0 \\ \hline
0 & 0 & 0 \\
\end{array}\right)_{n\times n}$

para $p,q\in\mathbb{Z},\quad 0\leq p,q \leq n$. En estas condiciones, a la métrica asociada a esa matriz se le suele denotar como $g_{p,q}$. Así:
\begin{itemize}
\item $g_{p,q}((x_1,\dots,x_n),(y_1,\dots,y_n))=x_1 y_1+\cdots+x_p y_p-x_{p+1} y_{p+1}-\cdots-x_{p+q} y_{p+q}$
\item $\omega_{g_{p,q}}((x_1,\dots,x_n))=x_1^2+\cdots+x_p^2-x_{p+1}^2-\cdots-x_{p+q}^2$
\end{itemize}
La métrica será de un tipo u otro según los valores de $p$ y $q$:
\begin{itemize}
\item $g_{p,q}$ es \textbf{degenerada} $\Leftrightarrow$ $p+q<n$.
\item $g_{p,q}$ es \textbf{no degenerada} $\Leftrightarrow$ $p+q=n$.
\item $g_{p,q}$ es \textbf{definida positiva} $\Leftrightarrow$ $p=n$ y $q=0$.
\item $g_{p,q}$ es \textbf{definida negativa} $\Leftrightarrow$ $p=0$ y $q=n$.
\item $g_{p,q}$ es \textbf{semidefinida positiva} $\Leftrightarrow$ $1\leq p< n$ y $q=0$.
\item $g_{p,q}$ es \textbf{semidefinida negativa} $\Leftrightarrow$ $p=0$ y $1\leq q<n$.
\item $g_{p,q}$ es \textbf{indefinida} $\Leftrightarrow$ $p\geq 1$ y $q\geq 1$.
\end{itemize}

Con este ejemplo podemos concluir que si una métrica tiene como matriz en cierta base a la matriz de este ejemplo, los valores de $p$ y $q$ nos indicarán de qué tipo es la métrica.

\textbf{Nota:} Para saber si la matriz de una métrica en cierta base es congruente con esta matriz podemos hacer transformaciones elementales por filas y columnas hasta obtener la matriz diagonal que nos dé información del tipo de la métrica. Se explicará con detalle más adelante con un ejemplo práctico.

\subsection{Métricas restringidas a subespacios}

Sea $(V,g)$ un espacio métrico y sea $U$ un subespacio vectorial de $V$. Se define la métrica restringida o inducida a $U$ y se denota $g_{|U}$ a la aplicación:
\begin{align*}
g_{|U}:U\times U &\longrightarrow\mathbb{R}\\
(\vec{u},\vec{v})&\longmapsto g_{|U}(\vec{u},\vec{v})=g(\vec{u},\vec{v})
\end{align*}
Y la forma cuadrática restringida a U es la aplicación:
\begin{align*}
\omega_{(g_{|U})}:U & \longrightarrow\mathbb{R}\\
\vec{u}& \longmapsto  g_{|U}(\vec{u},\vec{u})= g(\vec{u},\vec{u})=\omega_g(\vec{u})
\end{align*} 
Es decir, $\omega_{(g_{|U})}=(\omega_g)_{|U}$. La forma cuadrátrica asociada a la métrica inducida al subespacio es la forma cuadrática asociada, pero restringida al subespacio.
Las métricas, cuando su dominio y codominio se restringe a un subespacio vectorial pueden adquirir ciertas propiedades que quizá no tuvieran considerando todo $V\times V$ como el dominio.

\subsubsection*{Propiedades}

\begin{enumerate}
\item Dada $R=L(\{\vec{v}\})$ una recta de $V$, consideramos el subespacio métrico $(R,g_{|R})$. Como ${\omega_g}_{|R}(\lambda \vec{v})=\lambda^2\omega_g(\vec{v})\quad\forall\lambda\in\mathbb{R}$, entonces:
\begin{itemize}
\item $g_{|R}$ es euclídea $\Leftrightarrow \omega_g(\vec{v})>0$.
\item $g_{|R}$ es definida negativa $\Leftrightarrow \omega_g(\vec{v})<0$.
\item $g_{|R}=0\Leftrightarrow \omega_g(\vec{v})=0$.
\end{itemize} 
\item $g$ definida positiva y $U\not=\{0\}\Longrightarrow g_{|U}$ es definida positiva.
\item $g$ definida negativa y $U\not=\{0\}\Longrightarrow g_{|U}$ es definida negativa.
\item $g$ semidefinida positiva y $U\not=\{0\}\Longrightarrow g_{|U}$ es:
\begin{itemize}
\item O bien definida positiva ($\Leftrightarrow U\cap \rad(g)=\{0\}$)
\item O bien semidefinida positiva ($\Leftrightarrow \{0\}\not=(U\cap\rad(g))\subsetneq U$)
\item O bien es la métrica 0.
\end{itemize}
\item $g$ semidefinida negativa y $U\not=\{0\}\Longrightarrow g_{|U}$ es:
\begin{itemize}
\item O bien definida negativa ($\Leftrightarrow U\cap \rad(g)=\{0\}$)
\item O bien semidefinida negativa ($\Leftrightarrow \{0\}\not=(U\cap\rad(g))\subsetneq U$)
\item O bien es la métrica 0.
\end{itemize}
\item Si $g$ es indefinida, a priori no se puede afirmar nada sobre $g_{|U}$.
\item Si $g$ es degenerada y $U$  es un subespacio complementario de $\rad(g)\Rightarrow g_{|U}$ es no degenerada y $V=\rad(g)\obot U$

\textbf{Demostración:}
Partimos de que $V=U\oplus\rad(g)$, y como todo vector de $\rad(g)$ es perpendicular a todo vector de $V$, en particular es perpendicular a todo vector de $U$, luego $U\obot\rad(g)$. En conclusión, $V=U\obot\rad(g)$. 

Probemos ahora que $g_{|U}$ es no degenerada, para ello, calculemos su radical:
\[\rad(g_{|U})=\{\vec{u}\in U / \vec{u}\perp\vec{v}\quad\forall\vec{v}\in U\}\]
Tomamos $\vec{u}\in\rad(g_{|U})$ Por pertenecer al radical de $g_{|U}$, $\vec{u}\perp\vec{v}\quad\forall\vec{v}\in U$, y por pertenecer a $U$, $\vec{u}\perp\vec{w}\quad\forall\vec{w}\in\rad(g)$ por lo deducido anteriormente. Como $V=U\oplus\rad(g)$, entonces $\vec{u}\perp\vec{v}\quad\forall\vec{v}\in V$, equivalentemente,\linebreak $\vec{u}\in\rad(g)\cap U=\{0\}$, luego $\vec{u}=0$, es decir, $\rad(g_{|U})=\{0\}$ y $g_{|U}$ es no degenerada.
\end{enumerate}

\subsection{Espacios y subespacios ortogonales}
Dado $(V,g)$ un espacio vectorial métrico y $U$ un subespacio vectorial de $V$. Se define el espacio perpendicular u ortogonal a $U$ y se denota $U^\perp$ como el conjunto:
\[U^\perp=\{\vec{v}\in V/\vec{v}\perp\vec{u}\quad\forall\vec{u}\in U\}\]

\subsubsection{Propiedades}
\begin{enumerate}
\item $U^\perp$ es un subespacio vectorial de $V$,\quad$\forall\; U$ s.v. de $V$.
\item $\rad(g)\subseteq U^\perp\quad\forall\; U$ s.v. de $V$.
\item $V^\perp=\rad(g)$ y $\{0\}^\perp=V$.
\item Dado $\vec{v}\in V$, $v\not=0$, sea $R=L(\{\vec{v}\})$. Se verifica que $R^\perp=V$ ó $R^\perp$ es un hiperplano de $V$.
\item Si $\{\vec{u_1},\dots,\vec{u_k}\}$ es un sdg de $U\Rightarrow U^\perp=\left\lbrace\vec{v}\in V/\begin{array}{c}
\vec{v}\perp\vec{u_1}\\ \vdots \\ \vec{v}\perp\vec{u_k}
\end{array}\right\rbrace=\linebreak L(\{\vec{u_1}\})^\perp\cap\cdots\cap L(\{\vec{u_k}\})^\perp$
\end{enumerate}

\textbf{Demostración}
\begin{enumerate}

\item Dados $\vec{v_1},\vec{v_2}\in U^\perp$ y $\lambda,\mu\in U^\perp$:
\[
g(\lambda\vec{v_1}+\mu \vec{v_2},\vec{u})=\lambda \underbrace{g(\vec{v_1},\vec{u})}_{\vec{v_1}\in U^\perp,\vec{u}\in U}+\mu \underbrace{g(\vec{v_2},\vec{u})}_{\vec{v_2}\in U^\perp, \vec{u}\in U}=0\quad\forall\vec{u}\in U\]
Luego $\lambda\vec{v_1}+\mu \vec{v_2}\in U^\perp$

\item Dado $\vec{u}\in\rad(g)$, $\vec{u}$ es perpendicular a todo vector de $V$, en particular a todo vector de $U$, luego $\vec{u}\in U^\perp\quad\forall\vec{u}\in\rad(g)$, es decir  $\rad(g)\subseteq U^\perp$.
\item \[V^\perp=\{\vec{v}\in V/\vec{v}\perp\vec{u}\quad\forall\vec{u}\in V\}=\rad(g)\]
\[g(\vec{v},0)=0\quad\forall\vec{v}\in V\Rightarrow\{0\}^\perp=V\]
\item
\begin{itemize}
\item Si $\vec{v}\in\rad(g)\Rightarrow R^\perp=V$.
\item Si $\vec{v}\not\in\rad(g)$

Sea $B$ una base de $V$, $n=\dim(V)$ y sean $(a_1,\dots,a_n)_B$ las coordenadas de $\vec{v}$ en la base $B$.
\[R^\perp=\{\vec{u}\in V/\vec{u}\perp\vec{v}\}\]
Considerando que $\vec{u}=(x_1,\dots,x_n)_B$,
\[R^\perp=\left\lbrace \vec{u}\in V/\underbrace{\begin{pmatrix}
x_1 & \cdots & x_n
\end{pmatrix}\underbrace{M(g,B)\begin{pmatrix}
a_1 \\ \vdots \\ a_n
\end{pmatrix}}_{\not=0(\vec{v}\not\in\rad(g))}}_{\text{Una sola ecuación}}=0\right\rbrace \]
Como el subespacio tiene una sola ecuación, entonces $\dim(U^\perp)=n-1$, luego es un hiperplano.
\end{itemize}
\item Si $\{\vec{u_1},\dots,\vec{u_k}\}$ es un sdg de $U$, entonces, dado $\vec{u}\in U$, $\vec{u}=\sum_{i=1}^k \lambda_i\vec{u_i}$ para ciertos $\lambda_i, i\in\{1,\dots,k\}$. Por lo que, para cualquier $\vec{v}\in V$,\linebreak $g(\vec{u},\vec{v})=\lambda_1 g(\vec{u_1},\vec{v})+\cdots +\lambda_k g(\vec{u_k},\vec{v})=0\Leftrightarrow g(\vec{u_i},\vec{v})=0\Leftrightarrow\vec{u_i}\perp\vec{v}\linebreak\forall i\in\{1,\dots,k\}$

\end{enumerate}




\end{document}